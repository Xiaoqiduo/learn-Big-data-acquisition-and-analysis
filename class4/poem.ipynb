{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作诗"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "准备数据\n",
    "构建模型(构造损失函数和优化器)\n",
    "训练模型\n",
    "评估模型\n",
    "预测\n",
    "'''\n",
    "\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 禁用词\n",
    "disallowed_words = ['（', '）', '(', ')', '__', '《', '》', '【', '】', '[', ']']\n",
    "# 句子最大长度\n",
    "max_len = 64\n",
    "# 最小词频\n",
    "min_word_frequency = 8\n",
    "# mini batch 大小\n",
    "batch_size = 16\n",
    "\n",
    "# 加载数据集\n",
    "with open('./poetry.txt', 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    # 将冒号统一成相同格式\n",
    "    lines = [line.replace('：', ':') for line in lines]\n",
    "# 数据集列表\n",
    "poetry = []\n",
    "# 逐行处理读取到的数据\n",
    "for line in lines:\n",
    "    # 有且只能有一个冒号用来分割标题\n",
    "    if line.count(':') != 1:\n",
    "        continue\n",
    "    # 后半部分不能包含禁止词\n",
    "    __, last_part = line.split(':')\n",
    "#     print(__)\n",
    "    ignore_flag = False\n",
    "    for dis_word in disallowed_words:\n",
    "        if dis_word in last_part:\n",
    "            ignore_flag = True\n",
    "            break\n",
    "    if ignore_flag:\n",
    "        continue\n",
    "    # 长度不能超过最大长度\n",
    "    if len(last_part) > max_len - 2:\n",
    "        continue\n",
    "    poetry.append(last_part.replace('\\n', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编写一个类——Tokenizer，这是为了方便我们完成字符转编号、编号转字符、字符串转编号序列、编号序列转字符串等操作而编写的一个辅助类。\n",
    "\n",
    "使用的特殊字符有四个，为’[PAD]’, ‘[UNK]’, ‘[CLS]’, ‘[SEP]’，它们分别代表填充字符、低频词、古诗开始标记、古诗结束标记。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    \"\"\"\n",
    "    分词器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, token_dict):\n",
    "        # 词->编号的映射\n",
    "        self.token_dict = token_dict\n",
    "        # 编号->词的映射\n",
    "        self.token_dict_rev = {value: key for key, value in self.token_dict.items()}\n",
    "        # 词汇表大小\n",
    "        self.vocab_size = len(self.token_dict)\n",
    "\n",
    "    def id_to_token(self, token_id):\n",
    "        \"\"\"\n",
    "        给定一个编号，查找词汇表中对应的词\n",
    "        :param token_id: 带查找词的编号\n",
    "        :return: 编号对应的词\n",
    "        \"\"\"\n",
    "        return self.token_dict_rev[token_id]\n",
    "\n",
    "    def token_to_id(self, token):\n",
    "        \"\"\"\n",
    "        给定一个词，查找它在词汇表中的编号\n",
    "        未找到则返回低频词[UNK]的编号\n",
    "        :param token: 带查找编号的词\n",
    "        :return: 词的编号\n",
    "        \"\"\"\n",
    "        return self.token_dict.get(token, self.token_dict['[UNK]'])\n",
    "\n",
    "    def encode(self, tokens):\n",
    "        \"\"\"\n",
    "        给定一个字符串s，在头尾分别加上标记开始和结束的特殊字符，并将它转成对应的编号序列\n",
    "        :param tokens: 待编码字符串\n",
    "        :return: 编号序列\n",
    "        \"\"\"\n",
    "        # 加上开始标记\n",
    "        token_ids = [self.token_to_id('[CLS]'), ]\n",
    "        # 加入字符串编号序列\n",
    "        for token in tokens:\n",
    "            token_ids.append(self.token_to_id(token))\n",
    "        # 加上结束标记\n",
    "        token_ids.append(self.token_to_id('[SEP]'))\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        给定一个编号序列，将它解码成字符串\n",
    "        :param token_ids: 待解码的编号序列\n",
    "        :return: 解码出的字符串\n",
    "        \"\"\"\n",
    "        # 起止标记字符特殊处理\n",
    "        spec_tokens = {'[CLS]', '[SEP]'}\n",
    "        # 保存解码出的字符的list\n",
    "        tokens = []\n",
    "        for token_id in token_ids:\n",
    "            token = self.id_to_token(token_id)\n",
    "            if token in spec_tokens:\n",
    "                continue\n",
    "            tokens.append(token)\n",
    "        # 拼接字符串\n",
    "        return ''.join(tokens)\n",
    "\n",
    "\n",
    "# 统计词频\n",
    "counter = Counter()\n",
    "for line in poetry:\n",
    "    counter.update(line)\n",
    "# 过滤掉低频词\n",
    "_tokens = [(token, count) for token, count in counter.items() if count >= min_word_frequency]\n",
    "# 按词频排序\n",
    "_tokens = sorted(_tokens, key=lambda x: -x[1])\n",
    "# 去掉词频，只保留词列表\n",
    "_tokens = [token for token, count in _tokens]\n",
    "\n",
    "# 将特殊词和数据集中的词拼接起来\n",
    "_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]'] + _tokens\n",
    "# 创建词典 token->id映射关系\n",
    "token_id_dict = dict(zip(_tokens, range(len(_tokens))))\n",
    "# 使用新词典重新建立分词器\n",
    "tokenizer = Tokenizer(token_id_dict)\n",
    "# 混洗数据\n",
    "np.random.shuffle(poetry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "写成生成器的形式，主要出于内存方面的考虑。训练时需要对数据进行填充、转one-hot形式等操作，会占用较多内  存。如果提前对全部数据都进行处理，内存可能会溢出。而以生成器的形式，可以只在要进行训练的时候，处理相应  batch size的数据即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryDataGenerator:\n",
    "    \"\"\"\n",
    "    古诗数据集生成器\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, random=False):\n",
    "        # 数据集\n",
    "        self.data = data\n",
    "        # batch size\n",
    "        self.batch_size = batch_size\n",
    "        # 每个epoch迭代的步数\n",
    "        self.steps = int(math.floor(len(self.data) / self.batch_size))\n",
    "        # 每个epoch开始时是否随机混洗\n",
    "        self.random = random\n",
    "\n",
    "    def sequence_padding(self, data, length=None, padding=None):\n",
    "        \"\"\"\n",
    "        将给定数据填充到相同长度\n",
    "        :param data: 待填充数据\n",
    "        :param length: 填充后的长度，不传递此参数则使用data中的最大长度\n",
    "        :param padding: 用于填充的数据，不传递此参数则使用[PAD]的对应编号\n",
    "        :return: 填充后的数据\n",
    "        \"\"\"\n",
    "        # 计算填充长度\n",
    "        if length is None:\n",
    "            length = max(map(len, data))\n",
    "        # 计算填充数据\n",
    "        if padding is None:\n",
    "            padding = tokenizer.token_to_id('[PAD]')\n",
    "        # 开始填充\n",
    "        outputs = []\n",
    "        for line in data:\n",
    "            padding_length = length - len(line)\n",
    "            # 不足就进行填充\n",
    "            if padding_length > 0:\n",
    "                outputs.append(np.concatenate([line, [padding] * padding_length]))\n",
    "            # 超过就进行截断\n",
    "            else:\n",
    "                outputs.append(line[:length])\n",
    "        return np.array(outputs)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.steps\n",
    "\n",
    "    def __iter__(self):\n",
    "        total = len(self.data)\n",
    "        # 是否随机混洗\n",
    "        if self.random:\n",
    "            np.random.shuffle(self.data)\n",
    "        # 迭代一个epoch，每次yield一个batch\n",
    "        for start in range(0, total, self.batch_size):\n",
    "            end = min(start + self.batch_size, total)\n",
    "            batch_data = []\n",
    "            # 逐一对古诗进行编码\n",
    "            for single_data in self.data[start:end]:\n",
    "                batch_data.append(tokenizer.encode(single_data))\n",
    "            # 填充为相同长度\n",
    "            batch_data = self.sequence_padding(batch_data)\n",
    "            # yield x,y\n",
    "            #前面部分是数据x,后面部分是标签y。将诗的内容错开一位分别作为数据和标签。\n",
    "            #标签部分使用了one-hot进行处理，而数据部分没有使用。原因在于，数据部分准备输入词嵌入层，而词嵌入层的输入不需要进行one-hot；\n",
    "            #而标签部分，需要和模型的输出计算交叉熵，输出层的激活函数是softmax，所以标签部分也要转成相应的shape，故使用one-hot形式。\n",
    "            yield batch_data[:, :-1], tf.one_hot(batch_data[:, 1:], tokenizer.vocab_size)\n",
    "            del batch_data\n",
    "\n",
    "    def for_fit(self):\n",
    "        \"\"\"\n",
    "        创建一个生成器，用于训练\n",
    "        \"\"\"\n",
    "        # 死循环，当数据训练一个epoch之后，重新迭代数据\n",
    "        while True:\n",
    "            # 委托生成器\n",
    "            yield from self.__iter__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0106 06:41:50.551586 140464297342784 deprecation.py:506] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0106 06:41:50.581644 140464297342784 deprecation.py:506] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         439552    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, None, 128)         131584    \n",
      "_________________________________________________________________\n",
      "time_distributed (TimeDistri (None, None, 3434)        442986    \n",
      "=================================================================\n",
      "Total params: 1,145,706\n",
      "Trainable params: 1,145,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model = tf.keras.Sequential([\n",
    "    # 不定长度的输入\n",
    "    tf.keras.layers.Input((None,)),\n",
    "    # 词嵌入层\n",
    "    tf.keras.layers.Embedding(input_dim=tokenizer.vocab_size, output_dim=128),\n",
    "    # 第一个LSTM层，返回序列作为下一层的输入\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, return_sequences=True),\n",
    "    # 第二个LSTM层，返回序列作为下一层的输入\n",
    "    tf.keras.layers.LSTM(128, dropout=0.5, return_sequences=True),\n",
    "    # 对每一个时间点的输出都做softmax，预测下一个词的概率\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(tokenizer.vocab_size, activation='softmax')),\n",
    "])\n",
    "\n",
    "# 查看模型结构\n",
    "model.summary()\n",
    "# 配置优化器和损失函数\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.categorical_crossentropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_poetry(tokenizer, model, s=''):\n",
    "    \"\"\"\n",
    "    随机生成一首诗\n",
    "    :param tokenizer: 分词器\n",
    "    :param model: 用于生成古诗的模型\n",
    "    :param s: 用于生成古诗的起始字符串，默认为空串\n",
    "    :return: 一个字符串，表示一首古诗\n",
    "    \"\"\"\n",
    "    # 将初始字符串转成token\n",
    "    token_ids = tokenizer.encode(s)\n",
    "    # 去掉结束标记[SEP]\n",
    "    token_ids = token_ids[:-1]\n",
    "    while len(token_ids) < 64:\n",
    "        # 进行预测，只保留第一个样例（我们输入的样例数只有1）的、最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布\n",
    "        _probas = model.predict([token_ids, ])[0, -1, 3:]\n",
    "#         print(_probas)\n",
    "        # 按照出现概率，对所有token倒序排列\n",
    "        p_args = _probas.argsort()[::-1][:100]\n",
    "#         print(p_args)\n",
    "        # 排列后的概率顺序\n",
    "        p = _probas[p_args]\n",
    "#         print(p)\n",
    "        # 先对概率归一\n",
    "        p = p / sum(p)\n",
    "#         print(p)\n",
    "#         print(len(p))\n",
    "        # 再按照预测出的概率，随机选择一个词作为预测结果\n",
    "        target_index = np.random.choice(len(p), p=p)\n",
    "        target = p_args[target_index] + 3\n",
    "        # 保存\n",
    "        token_ids.append(target)\n",
    "        if target == 3:\n",
    "            break\n",
    "    return tokenizer.decode(token_ids)\n",
    "\n",
    "\n",
    "def generate_acrostic(tokenizer, model, head):\n",
    "    \"\"\"\n",
    "    随机生成一首藏头诗\n",
    "    :param tokenizer: 分词器\n",
    "    :param model: 用于生成古诗的模型\n",
    "    :param head: 藏头诗的头\n",
    "    :return: 一个字符串，表示一首古诗\n",
    "    \"\"\"\n",
    "    # 使用空串初始化token_ids，加入[CLS]\n",
    "    token_ids = tokenizer.encode('')\n",
    "    token_ids = token_ids[:-1]\n",
    "    # 标点符号，这里简单的只把逗号和句号作为标点\n",
    "    punctuations = ['，', '。']\n",
    "    punctuation_ids = {tokenizer.token_to_id(token) for token in punctuations}\n",
    "    # 缓存生成的诗的list\n",
    "    poetry = []\n",
    "    # 对于藏头诗中的每一个字，都生成一个短句\n",
    "    for ch in head:\n",
    "        # 先记录下这个字\n",
    "        poetry.append(ch)\n",
    "        # 将藏头诗的字符转成token id\n",
    "        token_id = tokenizer.token_to_id(ch)\n",
    "        # 加入到列表中去\n",
    "        token_ids.append(token_id)\n",
    "        # 开始生成一个短句\n",
    "        token_ids = tokenizer.encode('')\n",
    "        token_ids = token_ids[:-1]\n",
    "#         while len(token_ids) < 5:\n",
    "        while True:\n",
    "            # 进行预测，只保留第一个样例（我们输入的样例数只有1）的，最后一个token的预测的、不包含[PAD][UNK][CLS]的概率分布\n",
    "            _probas = model.predict([token_ids, ])[0, -1, 3:]\n",
    "            # 按照出现概率，对所有token倒序排列，只取前100\n",
    "            p_args = _probas.argsort()[::-1][:100]\n",
    "            # 排列后的概率顺序\n",
    "            p = _probas[p_args]\n",
    "            # 先对概率归一\n",
    "            p = p / sum(p)\n",
    "            # 再按照预测出的概率，随机选择一个词作为预测结果\n",
    "            target_index = np.random.choice(len(p), p=p)  # len(p)=100, p为概率\n",
    "            target = p_args[target_index] + 3\n",
    "#             print(target)\n",
    "            # 保存\n",
    "            token_ids.append(target)\n",
    "            # 只有不是特殊字符时，才保存到poetry里面去\n",
    "            if target > 3:\n",
    "                poetry.append(tokenizer.id_to_token(target))\n",
    "            if target in punctuation_ids:  # !!!!预测是下一个是标点符号才退出这次循环\n",
    "                break\n",
    "            if len(token_ids) % 5 ==0 :\n",
    "                poetry.append(' ')\n",
    "                break\n",
    "                \n",
    "    return ''.join(poetry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Evaluate(tf.keras.callbacks.Callback):\\n    \"\"\"\\n    在每个epoch训练完成后，保留最优权重，并随机生成settings.SHOW_NUM首古诗展示\\n    \"\"\"\\n\\n    def __init__(self):\\n        super().__init__()\\n        # 给loss赋一个较大的初始值\\n        self.lowest = 1e10\\n\\n    def on_epoch_end(self, epoch, logs=None):\\n        # 在每个epoch训练完成后调用\\n        # 如果当前loss更低，就保存当前模型参数\\n        if logs[\\'loss\\'] <= self.lowest:\\n            self.lowest = logs[\\'loss\\']\\n            model.save(\\'./best_model.h5\\')\\n        # 随机生成几首古体诗测试，查看训练效果\\n        print()\\n        for i in range(5):\\n            print(generate_random_poetry(tokenizer, model))\\n\\n\\n# 创建数据集\\ndata_generator = PoetryDataGenerator(poetry, random=True)\\n# 开始训练\\nmodel.fit_generator(data_generator.for_fit(), steps_per_epoch=data_generator.steps, epochs=20,\\n                    callbacks=[Evaluate()])\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#在训练时，我们只用随机生成古体诗的方法观察效果。在Keras里，可以通过回调（callback）执行测试方法。\n",
    "#训练的时间很长，大家看看即可，我们已经训练好了模型存储在best_model.h5\n",
    "'''\n",
    "class Evaluate(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    在每个epoch训练完成后，保留最优权重，并随机生成settings.SHOW_NUM首古诗展示\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 给loss赋一个较大的初始值\n",
    "        self.lowest = 1e10\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # 在每个epoch训练完成后调用\n",
    "        # 如果当前loss更低，就保存当前模型参数\n",
    "        if logs['loss'] <= self.lowest:\n",
    "            self.lowest = logs['loss']\n",
    "            model.save('./best_model.h5')\n",
    "        # 随机生成几首古体诗测试，查看训练效果\n",
    "        print()\n",
    "        for i in range(5):\n",
    "            print(generate_random_poetry(tokenizer, model))\n",
    "\n",
    "\n",
    "# 创建数据集\n",
    "data_generator = PoetryDataGenerator(poetry, random=True)\n",
    "# 开始训练\n",
    "model.fit_generator(data_generator.for_fit(), steps_per_epoch=data_generator.steps, epochs=20,\n",
    "                    callbacks=[Evaluate()])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "调用训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0106 06:41:53.935435 140464297342784 deprecation.py:323] From /opt/conda/lib/python3.6/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "modeltest = tf.keras.models.load_model('./best_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三雨春一今云白长柳欲南五仙见落独斋天西十海江楚三古南远独我衣一客白去朝玉人白西相几长沙新落楚相日紫楚衣晓二山春日出衣别一斋千南\n",
      "床前明月光，出行年旧千小今南欲君落玉旧高世天雨故西云去黄月君长闻一石野人行衣柳楚南三云金黄夜一古春草晓高秋碧一客为落二西君五红\n",
      "海南草君昔 阔平旧何欲 天旧花西野 空岁秋日江 \n",
      "教出云见一 育闲春野旧 技西雨旧仙 术君石夜秋 \n"
     ]
    }
   ],
   "source": [
    "# 随机生成一首诗\n",
    "print(generate_random_poetry(tokenizer, modeltest))\n",
    "# 给出部分信息的情况下，随机生成剩余部分\n",
    "print(generate_random_poetry(tokenizer, modeltest, s='床前明月光，'))\n",
    "# 生成藏头诗\n",
    "print(generate_acrostic(tokenizer, modeltest, head='海阔天空'))\n",
    "\n",
    "print(generate_acrostic(tokenizer, modeltest, head='教育技术'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
